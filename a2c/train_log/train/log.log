[33m[10-27 10:55:01 MainThread @visualdl.py:32][0m [5m[33mWRN[0m [VisualDL] logdir is None, will save VisualDL files to train_log/train
View the data using: visualdl --logdir=./train_log/train --host=172.20.34.172
[32m[10-27 10:55:01 MainThread @train.py:178][0m {'sample_steps': 68500, 'max_episode_rewards': 4710.688312354316, 'mean_episode_rewards': -2634.150731898165, 'min_episode_rewards': -6389.202201633367, 'max_episode_steps': 111, 'mean_episode_steps': 16.91479260369815, 'min_episode_steps': 4, 'total_loss': 1380506500.0, 'pi_loss': -276118.53, 'vf_loss': 2761565200.0, 'entropy': 458.4305, 'learn_time_s': 0.006201877593994141, 'elapsed_time_s': 10, 'lr': 0.00099315, 'entropy_coeff': -0.01}
[32m[10-27 10:55:05 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:55:11 MainThread @train.py:178][0m {'sample_steps': 161500, 'max_episode_rewards': 4113.034435913849, 'mean_episode_rewards': 2104.5282143474487, 'min_episode_rewards': -6202.453885437476, 'max_episode_steps': 106, 'mean_episode_steps': 60.14414996767938, 'min_episode_steps': 4, 'total_loss': 1409322500.0, 'pi_loss': 98825.26, 'vf_loss': 2818447400.0, 'entropy': 532.4578, 'learn_time_s': 0.006133432388305664, 'elapsed_time_s': 20, 'lr': 0.00098385, 'entropy_coeff': -0.01}
[32m[10-27 10:55:15 MainThread @train.py:224][0m Test reward:-3050.0 run times:4950.0
[32m[10-27 10:55:21 MainThread @train.py:178][0m {'sample_steps': 257500, 'max_episode_rewards': 3782.321556014389, 'mean_episode_rewards': 1933.3532783896926, 'min_episode_rewards': -5389.850106923981, 'max_episode_steps': 127, 'mean_episode_steps': 75.5173775671406, 'min_episode_steps': 4, 'total_loss': 931454400.0, 'pi_loss': 34029.98, 'vf_loss': 1862840700.0, 'entropy': 530.5078, 'learn_time_s': 0.006141140460968018, 'elapsed_time_s': 30, 'lr': 0.00097425, 'entropy_coeff': -0.01}
[32m[10-27 10:55:26 MainThread @train.py:224][0m Test reward:-3050.0 run times:4950.0
[32m[10-27 10:55:31 MainThread @train.py:178][0m {'sample_steps': 353000, 'max_episode_rewards': 3982.3992458396406, 'mean_episode_rewards': 2003.321788203162, 'min_episode_rewards': -5078.397051980688, 'max_episode_steps': 132, 'mean_episode_steps': 82.22164948453609, 'min_episode_steps': 4, 'total_loss': 888760260.0, 'pi_loss': 55110.297, 'vf_loss': 1777410200.0, 'entropy': 490.56384, 'learn_time_s': 0.006121397018432617, 'elapsed_time_s': 40, 'lr': 0.0009647, 'entropy_coeff': -0.01}
[32m[10-27 10:55:36 MainThread @train.py:224][0m Test reward:-3050.0 run times:4950.0
[32m[10-27 10:55:41 MainThread @train.py:178][0m {'sample_steps': 446000, 'max_episode_rewards': 4105.967044170129, 'mean_episode_rewards': 2217.4482656522578, 'min_episode_rewards': -5542.227174985761, 'max_episode_steps': 127, 'mean_episode_steps': 75.73436230706743, 'min_episode_steps': 4, 'total_loss': 1032857600.0, 'pi_loss': 79927.305, 'vf_loss': 2065555300.0, 'entropy': 422.9936, 'learn_time_s': 0.006392719745635986, 'elapsed_time_s': 50, 'lr': 0.0009554, 'entropy_coeff': -0.01}
[32m[10-27 10:55:47 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:55:51 MainThread @train.py:178][0m {'sample_steps': 537500, 'max_episode_rewards': 2769.5467088318055, 'mean_episode_rewards': 2226.4258351874855, 'min_episode_rewards': -4018.017565161465, 'max_episode_steps': 69, 'mean_episode_steps': 51.71291596164693, 'min_episode_steps': 5, 'total_loss': 1393179100.0, 'pi_loss': 146693.52, 'vf_loss': 2786065000.0, 'entropy': 126.58664, 'learn_time_s': 0.0063214278221130375, 'elapsed_time_s': 60, 'lr': 0.00094625, 'entropy_coeff': -0.01}
[32m[10-27 10:55:58 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:56:01 MainThread @train.py:178][0m {'sample_steps': 628000, 'max_episode_rewards': 2493.4664886139763, 'mean_episode_rewards': 2380.373626899362, 'min_episode_rewards': -1479.659927386645, 'max_episode_steps': 63, 'mean_episode_steps': 51.59109080525414, 'min_episode_steps': 28, 'total_loss': 1316249000.0, 'pi_loss': 77210.94, 'vf_loss': 2632343600.0, 'entropy': 32.379753, 'learn_time_s': 0.00633725643157959, 'elapsed_time_s': 70, 'lr': 0.0009372, 'entropy_coeff': -0.01}
[32m[10-27 10:56:09 MainThread @train.py:224][0m Test reward:2363.1297161390976 run times:10363.129716139098
[32m[10-27 10:56:11 MainThread @train.py:178][0m {'sample_steps': 718000, 'max_episode_rewards': 2481.2462469344173, 'mean_episode_rewards': 2396.10074383372, 'min_episode_rewards': 2331.8740469138065, 'max_episode_steps': 62, 'mean_episode_steps': 52.27858386535113, 'min_episode_steps': 45, 'total_loss': 1202535400.0, 'pi_loss': 43863.7, 'vf_loss': 2404983300.0, 'entropy': 16.257349, 'learn_time_s': 0.006362767219543457, 'elapsed_time_s': 80, 'lr': 0.0009282, 'entropy_coeff': -0.01}
[32m[10-27 10:56:20 MainThread @train.py:224][0m Test reward:2363.1297161390976 run times:10363.129716139098
[32m[10-27 10:56:21 MainThread @train.py:178][0m {'sample_steps': 810000, 'max_episode_rewards': 2480.0792303545277, 'mean_episode_rewards': 2378.612091189559, 'min_episode_rewards': 2331.8740469138065, 'max_episode_steps': 62, 'mean_episode_steps': 53.251736111111114, 'min_episode_steps': 46, 'total_loss': 1075238400.0, 'pi_loss': 21956.957, 'vf_loss': 2150432500.0, 'entropy': 8.674724, 'learn_time_s': 0.006359529495239258, 'elapsed_time_s': 90, 'lr': 0.0009190000000000001, 'entropy_coeff': -0.01}
[32m[10-27 10:56:31 MainThread @train.py:224][0m Test reward:2337.3082154583353 run times:10337.308215458335
[32m[10-27 10:56:31 MainThread @train.py:178][0m {'sample_steps': 900000, 'max_episode_rewards': 2481.1909203719188, 'mean_episode_rewards': 2399.4276168533383, 'min_episode_rewards': 2335.9254286588857, 'max_episode_steps': 62, 'mean_episode_steps': 50.80960451977401, 'min_episode_steps': 46, 'total_loss': 1139258500.0, 'pi_loss': 71857.66, 'vf_loss': 2278373600.0, 'entropy': 24.156425, 'learn_time_s': 0.006392185688018799, 'elapsed_time_s': 100, 'lr': 0.00091, 'entropy_coeff': -0.01}
[32m[10-27 10:56:41 MainThread @train.py:178][0m {'sample_steps': 991000, 'max_episode_rewards': 2480.9838661588856, 'mean_episode_rewards': 2390.5133937358323, 'min_episode_rewards': -4431.056668643015, 'max_episode_steps': 56, 'mean_episode_steps': 54.885473176612415, 'min_episode_steps': 20, 'total_loss': 908284700.0, 'pi_loss': 20089.408, 'vf_loss': 1816529400.0, 'entropy': 10.385157, 'learn_time_s': 0.006380894184112549, 'elapsed_time_s': 110, 'lr': 0.0009009, 'entropy_coeff': -0.01}
[32m[10-27 10:56:42 MainThread @train.py:224][0m Test reward:2462.1832154583353 run times:10462.183215458335
[32m[10-27 10:56:51 MainThread @train.py:178][0m {'sample_steps': 1083000, 'max_episode_rewards': 2476.0582154583353, 'mean_episode_rewards': 2415.588400754211, 'min_episode_rewards': 2339.9332154583353, 'max_episode_steps': 55, 'mean_episode_steps': 54.9976090854752, 'min_episode_steps': 54, 'total_loss': 633257860.0, 'pi_loss': 12630.05, 'vf_loss': 1266490400.0, 'entropy': 8.717504, 'learn_time_s': 0.006348092555999756, 'elapsed_time_s': 120, 'lr': 0.0008917, 'entropy_coeff': -0.01}
[32m[10-27 10:56:53 MainThread @train.py:224][0m Test reward:2365.0582154583353 run times:10365.058215458335
[32m[10-27 10:57:01 MainThread @train.py:178][0m {'sample_steps': 1175000, 'max_episode_rewards': 2473.4332154583353, 'mean_episode_rewards': 2381.4814544709784, 'min_episode_rewards': 2337.3082154583353, 'max_episode_steps': 56, 'mean_episode_steps': 55.35942203491872, 'min_episode_steps': 55, 'total_loss': 346776540.0, 'pi_loss': 4763.041, 'vf_loss': 693543550.0, 'entropy': 7.5595303, 'learn_time_s': 0.0063560748100280765, 'elapsed_time_s': 130, 'lr': 0.0008825, 'entropy_coeff': -0.01}
[32m[10-27 10:57:04 MainThread @train.py:224][0m Test reward:2390.1832154583353 run times:10390.183215458335
[32m[10-27 10:57:12 MainThread @train.py:178][0m {'sample_steps': 1266500, 'max_episode_rewards': 2431.8082154583353, 'mean_episode_rewards': 2391.8305532796326, 'min_episode_rewards': 2348.5582154583353, 'max_episode_steps': 56, 'mean_episode_steps': 56.0, 'min_episode_steps': 56, 'total_loss': 156270160.0, 'pi_loss': 486.5961, 'vf_loss': 312539360.0, 'entropy': 6.5005674, 'learn_time_s': 0.0063347148895263675, 'elapsed_time_s': 140, 'lr': 0.00087335, 'entropy_coeff': -0.01}
[32m[10-27 10:57:15 MainThread @train.py:224][0m Test reward:2376.3082154583353 run times:10376.308215458335
[32m[10-27 10:57:22 MainThread @train.py:178][0m {'sample_steps': 1357500, 'max_episode_rewards': 2470.8082154583353, 'mean_episode_rewards': 2383.3352005567294, 'min_episode_rewards': 2334.6832154583353, 'max_episode_steps': 62, 'mean_episode_steps': 56.09068476249229, 'min_episode_steps': 56, 'total_loss': 54940924.0, 'pi_loss': -1271.6809, 'vf_loss': 109884380.0, 'entropy': 6.1512623, 'learn_time_s': 0.006254861354827881, 'elapsed_time_s': 150, 'lr': 0.00086425, 'entropy_coeff': -0.01}
[32m[10-27 10:57:26 MainThread @train.py:224][0m Test reward:2417.9332154583353 run times:10417.933215458335
[32m[10-27 10:57:32 MainThread @train.py:178][0m {'sample_steps': 1449500, 'max_episode_rewards': 2473.4332154583353, 'mean_episode_rewards': 2421.773530899989, 'min_episode_rewards': 2334.6832154583353, 'max_episode_steps': 62, 'mean_episode_steps': 56.21624923640806, 'min_episode_steps': 55, 'total_loss': 14017920.0, 'pi_loss': -867.44507, 'vf_loss': 28037576.0, 'entropy': 6.459486, 'learn_time_s': 0.006273956298828125, 'elapsed_time_s': 160, 'lr': 0.00085505, 'entropy_coeff': -0.01}
[32m[10-27 10:57:38 MainThread @train.py:224][0m Test reward:2470.8082154583353 run times:10470.808215458335
[32m[10-27 10:57:42 MainThread @train.py:178][0m {'sample_steps': 1527000, 'max_episode_rewards': 2470.8082154583353, 'mean_episode_rewards': 2409.100465599924, 'min_episode_rewards': 2334.6832154583353, 'max_episode_steps': 62, 'mean_episode_steps': 56.23892519970951, 'min_episode_steps': 56, 'total_loss': 4361584.5, 'pi_loss': -656.53485, 'vf_loss': 8724482.0, 'entropy': 5.0812154, 'learn_time_s': 0.007303984165191651, 'elapsed_time_s': 170, 'lr': 0.0008472999999999999, 'entropy_coeff': -0.01}
[32m[10-27 10:57:51 MainThread @train.py:224][0m Test reward:2404.0582154583353 run times:10404.058215458335
[32m[10-27 10:57:52 MainThread @train.py:178][0m {'sample_steps': 1602500, 'max_episode_rewards': 2445.6832154583353, 'mean_episode_rewards': 2404.6547684605594, 'min_episode_rewards': 2348.5582154583353, 'max_episode_steps': 56, 'mean_episode_steps': 56.0, 'min_episode_steps': 56, 'total_loss': 1477800.8, 'pi_loss': -488.13254, 'vf_loss': 2956578.0, 'entropy': 5.9817805, 'learn_time_s': 0.007081351280212402, 'elapsed_time_s': 180, 'lr': 0.00083975, 'entropy_coeff': -0.01}
[32m[10-27 10:58:02 MainThread @train.py:178][0m {'sample_steps': 1682500, 'max_episode_rewards': 2470.8082154583353, 'mean_episode_rewards': 2392.335320721493, 'min_episode_rewards': 2334.6832154583353, 'max_episode_steps': 57, 'mean_episode_steps': 56.17052631578947, 'min_episode_steps': 56, 'total_loss': 793546.0, 'pi_loss': -530.1776, 'vf_loss': 1588152.5, 'entropy': 5.1886797, 'learn_time_s': 0.0071674704551696776, 'elapsed_time_s': 190, 'lr': 0.00083175, 'entropy_coeff': -0.01}
[32m[10-27 10:58:04 MainThread @train.py:224][0m Test reward:2348.5582154583353 run times:10348.558215458335
[32m[10-27 10:58:12 MainThread @train.py:178][0m {'sample_steps': 1769500, 'max_episode_rewards': 2470.8082154583353, 'mean_episode_rewards': 2419.85114228559, 'min_episode_rewards': 2334.6832154583353, 'max_episode_steps': 62, 'mean_episode_steps': 56.611581001951855, 'min_episode_steps': 56, 'total_loss': 593520.0, 'pi_loss': -223.84206, 'vf_loss': 1187487.6, 'entropy': 1.4456627, 'learn_time_s': 0.006790368556976318, 'elapsed_time_s': 200, 'lr': 0.0008230500000000001, 'entropy_coeff': -0.01}
[32m[10-27 10:58:15 MainThread @train.py:224][0m Test reward:2470.8082154583353 run times:10470.808215458335
[32m[10-27 10:58:22 MainThread @train.py:178][0m {'sample_steps': 1857500, 'max_episode_rewards': 2470.8082154583353, 'mean_episode_rewards': 2440.237998045524, 'min_episode_rewards': 2334.6832154583353, 'max_episode_steps': 62, 'mean_episode_steps': 56.80297157622739, 'min_episode_steps': 56, 'total_loss': 571010.1, 'pi_loss': -487.08133, 'vf_loss': 1142994.4, 'entropy': 2.622579, 'learn_time_s': 0.006801717281341553, 'elapsed_time_s': 210, 'lr': 0.0008142500000000001, 'entropy_coeff': -0.01}
[32m[10-27 10:58:26 MainThread @train.py:224][0m Test reward:2390.1832154583353 run times:10390.183215458335
[32m[10-27 10:58:32 MainThread @train.py:178][0m {'sample_steps': 1947000, 'max_episode_rewards': 2470.8082154583353, 'mean_episode_rewards': 2404.1485782882087, 'min_episode_rewards': 2334.6832154583353, 'max_episode_steps': 62, 'mean_episode_steps': 58.04860661049903, 'min_episode_steps': 56, 'total_loss': 1003864.06, 'pi_loss': -138.89018, 'vf_loss': 2008006.1, 'entropy': 1.8805345, 'learn_time_s': 0.006650395393371582, 'elapsed_time_s': 220, 'lr': 0.0008053, 'entropy_coeff': -0.01}
[32m[10-27 10:58:38 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:58:42 MainThread @train.py:178][0m {'sample_steps': 2037500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 236571.98, 'pi_loss': 1.1006836e-06, 'vf_loss': 473143.97, 'entropy': 7.109583e-06, 'learn_time_s': 0.006263637542724609, 'elapsed_time_s': 230, 'lr': 0.0007962500000000001, 'entropy_coeff': -0.01}
[32m[10-27 10:58:49 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:58:52 MainThread @train.py:178][0m {'sample_steps': 2131000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 206075.45, 'pi_loss': 5.095877e-07, 'vf_loss': 412150.9, 'entropy': 6.7995466e-06, 'learn_time_s': 0.006318337917327881, 'elapsed_time_s': 240, 'lr': 0.0007869, 'entropy_coeff': -0.01}
[32m[10-27 10:58:59 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:59:02 MainThread @train.py:178][0m {'sample_steps': 2223000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 173453.77, 'pi_loss': 1.4242087e-07, 'vf_loss': 346907.53, 'entropy': 6.4445285e-06, 'learn_time_s': 0.006325428485870361, 'elapsed_time_s': 250, 'lr': 0.0007777000000000001, 'entropy_coeff': -0.01}
[32m[10-27 10:59:10 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:59:12 MainThread @train.py:178][0m {'sample_steps': 2315000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 131549.8, 'pi_loss': 0.0, 'vf_loss': 263099.6, 'entropy': 5.7234924e-06, 'learn_time_s': 0.006322634220123291, 'elapsed_time_s': 260, 'lr': 0.0007685, 'entropy_coeff': -0.01}
[32m[10-27 10:59:21 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:59:22 MainThread @train.py:178][0m {'sample_steps': 2407500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 111779.07, 'pi_loss': 0.0, 'vf_loss': 223558.14, 'entropy': 5.0906738e-06, 'learn_time_s': 0.006356492042541504, 'elapsed_time_s': 270, 'lr': 0.0007592499999999999, 'entropy_coeff': -0.01}
[32m[10-27 10:59:32 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:59:32 MainThread @train.py:178][0m {'sample_steps': 2500000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 99608.29, 'pi_loss': 0.0, 'vf_loss': 199216.58, 'entropy': 4.7402536e-06, 'learn_time_s': 0.006379127502441406, 'elapsed_time_s': 280, 'lr': 0.00075, 'entropy_coeff': -0.01}
[32m[10-27 10:59:42 MainThread @train.py:178][0m {'sample_steps': 2592000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 87149.68, 'pi_loss': 0.0, 'vf_loss': 174299.36, 'entropy': 4.3389805e-06, 'learn_time_s': 0.006319806575775146, 'elapsed_time_s': 290, 'lr': 0.0007408, 'entropy_coeff': -0.01}
[32m[10-27 10:59:43 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 10:59:52 MainThread @train.py:178][0m {'sample_steps': 2684000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 62368.36, 'pi_loss': 0.0, 'vf_loss': 124736.72, 'entropy': 3.878963e-06, 'learn_time_s': 0.006338002681732177, 'elapsed_time_s': 300, 'lr': 0.0007316, 'entropy_coeff': -0.01}
[32m[10-27 10:59:54 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:00:02 MainThread @train.py:178][0m {'sample_steps': 2776500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 57293.203, 'pi_loss': 0.0, 'vf_loss': 114586.41, 'entropy': 3.5320224e-06, 'learn_time_s': 0.006327540874481201, 'elapsed_time_s': 310, 'lr': 0.0007223500000000001, 'entropy_coeff': -0.01}
[32m[10-27 11:00:05 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:00:12 MainThread @train.py:178][0m {'sample_steps': 2868500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 46551.24, 'pi_loss': 0.0, 'vf_loss': 93102.48, 'entropy': 3.222828e-06, 'learn_time_s': 0.006299707889556885, 'elapsed_time_s': 321, 'lr': 0.00071315, 'entropy_coeff': -0.01}
[32m[10-27 11:00:16 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:00:22 MainThread @train.py:178][0m {'sample_steps': 2958500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 40154.11, 'pi_loss': 0.0, 'vf_loss': 80308.22, 'entropy': 2.985537e-06, 'learn_time_s': 0.0063144278526306155, 'elapsed_time_s': 331, 'lr': 0.0007041500000000001, 'entropy_coeff': -0.01}
[32m[10-27 11:00:27 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:00:32 MainThread @train.py:178][0m {'sample_steps': 3051000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 33934.14, 'pi_loss': 0.0, 'vf_loss': 67868.28, 'entropy': 2.6416926e-06, 'learn_time_s': 0.006389188766479492, 'elapsed_time_s': 341, 'lr': 0.0006949000000000001, 'entropy_coeff': -0.01}
[32m[10-27 11:00:38 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:00:42 MainThread @train.py:178][0m {'sample_steps': 3143500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 25836.963, 'pi_loss': 0.0, 'vf_loss': 51673.926, 'entropy': 2.48625e-06, 'learn_time_s': 0.006324496269226074, 'elapsed_time_s': 351, 'lr': 0.00068565, 'entropy_coeff': -0.01}
[32m[10-27 11:00:48 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:00:52 MainThread @train.py:178][0m {'sample_steps': 3237000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 22411.443, 'pi_loss': 0.0, 'vf_loss': 44822.887, 'entropy': 2.1833423e-06, 'learn_time_s': 0.006315851211547851, 'elapsed_time_s': 361, 'lr': 0.0006763, 'entropy_coeff': -0.01}
[32m[10-27 11:00:59 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:01:02 MainThread @train.py:178][0m {'sample_steps': 3326000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 18939.883, 'pi_loss': 0.0, 'vf_loss': 37879.766, 'entropy': 2.019969e-06, 'learn_time_s': 0.006658759117126465, 'elapsed_time_s': 371, 'lr': 0.0006674, 'entropy_coeff': -0.01}
[32m[10-27 11:01:11 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:01:12 MainThread @train.py:178][0m {'sample_steps': 3416500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 16975.072, 'pi_loss': 0.0, 'vf_loss': 33950.145, 'entropy': 1.878998e-06, 'learn_time_s': 0.006358706951141357, 'elapsed_time_s': 381, 'lr': 0.00065835, 'entropy_coeff': -0.01}
[32m[10-27 11:01:21 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:01:22 MainThread @train.py:178][0m {'sample_steps': 3508500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 13378.684, 'pi_loss': 0.0, 'vf_loss': 26757.367, 'entropy': 1.7833023e-06, 'learn_time_s': 0.006398494243621826, 'elapsed_time_s': 391, 'lr': 0.00064915, 'entropy_coeff': -0.01}
[32m[10-27 11:01:32 MainThread @train.py:178][0m {'sample_steps': 3598500, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 15240.213, 'pi_loss': 0.0, 'vf_loss': 30480.426, 'entropy': 1.7219157e-06, 'learn_time_s': 0.00635483980178833, 'elapsed_time_s': 401, 'lr': 0.00064015, 'entropy_coeff': -0.01}
[32m[10-27 11:01:33 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
[32m[10-27 11:01:43 MainThread @train.py:178][0m {'sample_steps': 3690000, 'max_episode_rewards': 2426.6760599550908, 'mean_episode_rewards': 2426.6760599550908, 'min_episode_rewards': 2426.6760599550908, 'max_episode_steps': 62, 'mean_episode_steps': 62.0, 'min_episode_steps': 62, 'total_loss': 15143.941, 'pi_loss': 0.0, 'vf_loss': 30287.883, 'entropy': 1.5986798e-06, 'learn_time_s': 0.006412532329559326, 'elapsed_time_s': 411, 'lr': 0.000631, 'entropy_coeff': -0.01}
[32m[10-27 11:01:44 MainThread @train.py:224][0m Test reward:2426.6760599550908 run times:10426.67605995509
